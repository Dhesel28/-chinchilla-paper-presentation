{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinchilla Scaling Laws - Code Demonstration\n",
    "## Training Compute-Optimal Large Language Models\n",
    "\n",
    "This notebook demonstrates the key scaling relationships discovered in the Chinchilla paper.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Dhesel28/-chinchilla-paper-presentation/blob/main/chinchilla_scaling_demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install numpy matplotlib scipy pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Parametric Loss Function\n",
    "\n",
    "Chinchilla models loss as:\n",
    "\n",
    "$$L(N, D) = E + \\frac{A}{N^\\alpha} + \\frac{B}{D^\\beta}$$\n",
    "\n",
    "Where:\n",
    "- $N$ = number of parameters\n",
    "- $D$ = number of training tokens\n",
    "- $E$ = irreducible loss (entropy of natural text)\n",
    "- $\\alpha, \\beta$ = scaling exponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinchilla's fitted parameters\n",
    "E = 1.69\n",
    "A = 406.4\n",
    "B = 410.7\n",
    "alpha = 0.34\n",
    "beta = 0.28\n",
    "\n",
    "def loss_function(N, D):\n",
    "    \"\"\"Compute predicted loss given model size N and training tokens D\"\"\"\n",
    "    return E + A / (N ** alpha) + B / (D ** beta)\n",
    "\n",
    "# Example: Compute loss for different model/data configurations\n",
    "print(\"Loss predictions:\")\n",
    "print(f\"Gopher (280B params, 300B tokens): {loss_function(280e9, 300e9):.4f}\")\n",
    "print(f\"Chinchilla (70B params, 1.4T tokens): {loss_function(70e9, 1.4e12):.4f}\")\n",
    "print(f\"GPT-3 (175B params, 300B tokens): {loss_function(175e9, 300e9):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimal Scaling Relationships\n",
    "\n",
    "For a given compute budget $C$, the optimal model size and training tokens are:\n",
    "\n",
    "$$N_{opt} \\propto C^{0.50}$$\n",
    "$$D_{opt} \\propto C^{0.49}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_flops(N, D):\n",
    "    \"\"\"Approximate FLOPs for training (simplified)\"\"\"\n",
    "    return 6 * N * D\n",
    "\n",
    "def optimal_model_size(C, a=0.50, G=0.5):\n",
    "    \"\"\"Compute optimal model size for compute budget C\"\"\"\n",
    "    return G * (C / 6) ** a\n",
    "\n",
    "def optimal_tokens(C, b=0.49, H=2.0):\n",
    "    \"\"\"Compute optimal training tokens for compute budget C\"\"\"\n",
    "    return H * (C / 6) ** b\n",
    "\n",
    "# Generate compute budgets from 1e20 to 1e25 FLOPs\n",
    "compute_budgets = np.logspace(20, 25, 50)\n",
    "\n",
    "# Compute optimal allocations\n",
    "optimal_params = [optimal_model_size(C) for C in compute_budgets]\n",
    "optimal_data = [optimal_tokens(C) for C in compute_budgets]\n",
    "\n",
    "# Create comparison dataframe\n",
    "models_df = pd.DataFrame({\n",
    "    'Model': ['GPT-3', 'Gopher', 'Chinchilla', 'MT-NLG'],\n",
    "    'Parameters': [175e9, 280e9, 70e9, 530e9],\n",
    "    'Tokens': [300e9, 300e9, 1.4e12, 270e9],\n",
    "    'Compute': [175e9*300e9*6, 280e9*300e9*6, 70e9*1.4e12*6, 530e9*270e9*6]\n",
    "})\n",
    "\n",
    "print(\"\\nExisting Large Models:\")\n",
    "print(models_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing Chinchilla vs Kaplan Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaplan et al. (2020) scaling laws\n",
    "def kaplan_model_size(C, a=0.73):\n",
    "    return 0.5 * (C / 6) ** a\n",
    "\n",
    "def kaplan_tokens(C, b=0.27):\n",
    "    return 2.0 * (C / 6) ** b\n",
    "\n",
    "# Compute Kaplan predictions\n",
    "kaplan_params = [kaplan_model_size(C) for C in compute_budgets]\n",
    "kaplan_data = [kaplan_tokens(C) for C in compute_budgets]\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Optimal Parameters vs Compute\n",
    "ax1.loglog(compute_budgets, optimal_params, 'b-', linewidth=2, label='Chinchilla (a=0.50)')\n",
    "ax1.loglog(compute_budgets, kaplan_params, 'r--', linewidth=2, label='Kaplan et al. (a=0.73)')\n",
    "\n",
    "# Add existing models\n",
    "ax1.scatter(models_df['Compute'], models_df['Parameters'], \n",
    "           c=['red', 'orange', 'green', 'purple'], s=200, zorder=5, edgecolor='black')\n",
    "for idx, row in models_df.iterrows():\n",
    "    ax1.annotate(row['Model'], (row['Compute'], row['Parameters']), \n",
    "                xytext=(10, 10), textcoords='offset points', fontsize=10)\n",
    "\n",
    "ax1.set_xlabel('Training FLOPs', fontsize=14)\n",
    "ax1.set_ylabel('Optimal Model Size (Parameters)', fontsize=14)\n",
    "ax1.set_title('Optimal Model Size vs Compute Budget', fontsize=16)\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Optimal Tokens vs Compute\n",
    "ax2.loglog(compute_budgets, optimal_data, 'b-', linewidth=2, label='Chinchilla (b=0.49)')\n",
    "ax2.loglog(compute_budgets, kaplan_data, 'r--', linewidth=2, label='Kaplan et al. (b=0.27)')\n",
    "\n",
    "# Add existing models\n",
    "ax2.scatter(models_df['Compute'], models_df['Tokens'], \n",
    "           c=['red', 'orange', 'green', 'purple'], s=200, zorder=5, edgecolor='black')\n",
    "for idx, row in models_df.iterrows():\n",
    "    ax2.annotate(row['Model'], (row['Compute'], row['Tokens']), \n",
    "                xytext=(10, 10), textcoords='offset points', fontsize=10)\n",
    "\n",
    "ax2.set_xlabel('Training FLOPs', fontsize=14)\n",
    "ax2.set_ylabel('Optimal Training Tokens', fontsize=14)\n",
    "ax2.set_title('Optimal Training Tokens vs Compute Budget', fontsize=16)\n",
    "ax2.legend(fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Key Insight: Chinchilla predicts balanced scaling (50/50), while Kaplan heavily favored model size (73/27)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Contour Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of model sizes and token counts\n",
    "N_range = np.logspace(8, 12, 100)  # 100M to 1T parameters\n",
    "D_range = np.logspace(10, 13, 100)  # 10B to 10T tokens\n",
    "N_grid, D_grid = np.meshgrid(N_range, D_range)\n",
    "\n",
    "# Compute loss for each point\n",
    "L_grid = loss_function(N_grid, D_grid)\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "levels = np.linspace(1.8, 3.0, 25)\n",
    "contour = plt.contour(N_grid, D_grid, L_grid, levels=levels, cmap='viridis')\n",
    "plt.clabel(contour, inline=True, fontsize=8, fmt='%.2f')\n",
    "\n",
    "# Plot efficient frontier (iso-FLOP lines)\n",
    "for C in [1e22, 1e23, 5.76e23, 1e24, 1e25]:\n",
    "    N_opt = optimal_model_size(C)\n",
    "    D_opt = optimal_tokens(C)\n",
    "    plt.scatter([N_opt], [D_opt], c='red', s=100, zorder=5, edgecolor='white', linewidth=2)\n",
    "    \n",
    "    # Draw iso-FLOP line\n",
    "    N_isoflop = np.logspace(8, 12, 50)\n",
    "    D_isoflop = C / (6 * N_isoflop)\n",
    "    plt.plot(N_isoflop, D_isoflop, 'r--', alpha=0.5, linewidth=1)\n",
    "\n",
    "# Add existing models\n",
    "for idx, row in models_df.iterrows():\n",
    "    plt.scatter([row['Parameters']], [row['Tokens']], \n",
    "               s=300, zorder=6, edgecolor='black', linewidth=2)\n",
    "    plt.annotate(row['Model'], (row['Parameters'], row['Tokens']), \n",
    "                xytext=(15, 15), textcoords='offset points', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Model Size (Parameters)', fontsize=14)\n",
    "plt.ylabel('Training Tokens', fontsize=14)\n",
    "plt.title('Loss Contours and Compute-Optimal Frontier', fontsize=16)\n",
    "plt.colorbar(contour, label='Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Red dots show the compute-optimal frontier. Existing models are undertrained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Calculator: Given Your Compute Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_model_config(compute_budget_flops):\n",
    "    \"\"\"\n",
    "    Given a compute budget in FLOPs, recommend optimal model size and training tokens\n",
    "    \"\"\"\n",
    "    N_opt = optimal_model_size(compute_budget_flops)\n",
    "    D_opt = optimal_tokens(compute_budget_flops)\n",
    "    predicted_loss = loss_function(N_opt, D_opt)\n",
    "    \n",
    "    print(f\"=\"*60)\n",
    "    print(f\"Compute Budget: {compute_budget_flops:.2e} FLOPs\")\n",
    "    print(f\"=\"*60)\n",
    "    print(f\"Optimal Model Size: {N_opt/1e9:.2f}B parameters\")\n",
    "    print(f\"Optimal Training Tokens: {D_opt/1e9:.2f}B tokens\")\n",
    "    print(f\"Predicted Loss: {predicted_loss:.4f}\")\n",
    "    print(f\"=\"*60)\n",
    "    \n",
    "    # Compare to Kaplan\n",
    "    N_kaplan = kaplan_model_size(compute_budget_flops)\n",
    "    D_kaplan = kaplan_tokens(compute_budget_flops)\n",
    "    \n",
    "    print(f\"\\nKaplan et al. (2020) would recommend:\")\n",
    "    print(f\"  Model Size: {N_kaplan/1e9:.2f}B parameters ({N_kaplan/N_opt:.2f}x larger)\")\n",
    "    print(f\"  Training Tokens: {D_kaplan/1e9:.2f}B tokens ({D_opt/D_kaplan:.2f}x more data needed!)\")\n",
    "    \n",
    "    return N_opt, D_opt\n",
    "\n",
    "# Example: What if you have the same budget as Gopher?\n",
    "gopher_compute = 5.76e23\n",
    "recommend_model_config(gopher_compute)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRY IT YOURSELF: Change the compute budget below!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Interactive: Try different budgets\n",
    "my_compute_budget = 1e24  # Change this value!\n",
    "recommend_model_config(my_compute_budget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Token-to-Parameter Ratio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare token-to-parameter ratios\n",
    "models_comparison = pd.DataFrame({\n",
    "    'Model': ['GPT-3', 'Gopher', 'Chinchilla', 'LLaMA-1 (65B)', 'LLaMA-2 (70B)', 'LLaMA-3 (8B)'],\n",
    "    'Parameters (B)': [175, 280, 70, 65, 70, 8],\n",
    "    'Tokens (B)': [300, 300, 1400, 1400, 2000, 15000],\n",
    "})\n",
    "\n",
    "models_comparison['Token:Param Ratio'] = models_comparison['Tokens (B)'] / models_comparison['Parameters (B)']\n",
    "models_comparison['Chinchilla Optimal?'] = models_comparison['Token:Param Ratio'].apply(\n",
    "    lambda x: 'âœ“ Optimal' if 15 <= x <= 25 else ('âš  Overtrained' if x > 25 else 'âœ— Undertrained')\n",
    ")\n",
    "\n",
    "print(\"\\nToken-to-Parameter Ratio Comparison:\")\n",
    "print(models_comparison.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['red' if 'âœ—' in status else 'orange' if 'âš ' in status else 'green' \n",
    "          for status in models_comparison['Chinchilla Optimal?']]\n",
    "\n",
    "plt.barh(models_comparison['Model'], models_comparison['Token:Param Ratio'], color=colors, alpha=0.7)\n",
    "plt.axvline(x=20, color='green', linestyle='--', linewidth=2, label='Chinchilla Optimal (~20:1)')\n",
    "plt.xlabel('Token-to-Parameter Ratio', fontsize=14)\n",
    "plt.ylabel('Model', fontsize=14)\n",
    "plt.title('Token-to-Parameter Ratios Across Large Language Models', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "### The Chinchilla Scaling Law:\n",
    "1. **Equal scaling**: For every doubling of compute, double BOTH model size and training tokens\n",
    "2. **Previous models were undertrained**: GPT-3, Gopher, etc. used ~4Ã— too many parameters and ~4Ã— too little data\n",
    "3. **Practical impact**: \n",
    "   - Same compute budget â†’ Better performance with smaller, longer-trained models\n",
    "   - 4Ã— reduction in inference costs\n",
    "   - Much more emphasis on data quality and quantity\n",
    "\n",
    "### Implications:\n",
    "- **Training**: Spend more time collecting and curating data\n",
    "- **Deployment**: Smaller models are cheaper to serve\n",
    "- **Research**: Data scaling is as important as model scaling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
